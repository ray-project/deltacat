{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Working with Rivulet Datasets\n",
    "This demo showcases\n",
    "1. How to create a dataset from a Parquet file using Deltacat.\n",
    "2. How to dynamically modify a dataset schema by adding new columns.\n",
    "3. How to append new rows and update existing rows without altering the original data files.\n",
    "4. How to query and read data from the updated dataset efficiently."
   ],
   "id": "637f8ea2826c9c06"
  },
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "code",
   "source": [
    "import deltacat as dc\n",
    "import pathlib\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Step 1: Create a simple 3x3 Parquet file using pyarrow",
   "id": "16b8c0d0d1bd7350"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "parquet_file_path = pathlib.Path.cwd() / \"contacts.parquet\"\n",
    "data = {\n",
    "    \"id\": [1, 2, 3],\n",
    "    \"name\": [\"Alice\", \"Bob\", \"Charlie\"],\n",
    "    \"age\": [25, 30, 35]\n",
    "}\n",
    "table = pa.Table.from_pydict(data)\n",
    "pq.write_table(table, parquet_file_path)\n",
    "print(f\"Created Parquet file at: {parquet_file_path}\")"
   ],
   "id": "7ab6a3460b6936ee",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Step 2: Load the Parquet file into a Dataset",
   "id": "ee89d76661a466ef"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "dataset = dc.Dataset.from_parquet(\n",
    "    name=\"contacts\",\n",
    "    file_uri=parquet_file_path,\n",
    "    metadata_uri=\".\",\n",
    "    merge_keys=\"id\"\n",
    ")\n",
    "print(\"Loaded dataset from Parquet file.\")\n",
    "dataset.print()"
   ],
   "id": "a2459fbc7dbf5053",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Step 3: Add two new fields to the Dataset",
   "id": "ec9db8f54290095b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "dataset.add_fields([\n",
    "    (\"id\", dc.Datatype.int64()),\n",
    "    (\"email\", dc.Datatype.string()),\n",
    "    (\"is_active\", dc.Datatype.bool())\n",
    "], schema_name=\"updated_schema\", merge_keys=[\"id\"])\n",
    "print(\"Added 'email' and 'is_active' fields to the dataset schema.\")"
   ],
   "id": "d5c56d43cb93ded6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Step 4: Append two new records\n",
    "The cool thing with deltacat datasets is that deltacat will not attempt to\n",
    "rewrite the existing Parquet file; instead, they will store additional data\n",
    "files alongside the original Parquet file(s) that can be easily joined with the originals."
   ],
   "id": "a40499cca02443bf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "dataset_writer = dataset.writer(file_format=\"feather\", schema_name=\"updated_schema\")\n",
    "\n",
    "# Define some new rows w/ the expanded schema and write them\n",
    "new_rows = [\n",
    "    {\"id\": 4, \"name\": \"David\",   \"age\": 40, \"email\": \"david@example.com\", \"is_active\": True},\n",
    "    {\"id\": 5, \"name\": \"Eve\",     \"age\": 45, \"email\": \"eve@example.com\",   \"is_active\": False}\n",
    "]\n",
    "dataset_writer.write(new_rows)\n",
    "print(\"Wrote 2 new rows (records) with expanded schema.\")\n",
    "\n",
    "# Write into the new columns on existing rows and write them, again without modifying/messing with the original parquet file.\n",
    "updates_for_existing_rows = [\n",
    "    {\"id\": 3, \"email\": \"charlie@example.com\", \"is_active\": True},\n",
    "    {\"id\": 2, \"email\": \"bob@example.com\",     \"is_active\": False},\n",
    "    {\"id\": 1, \"email\": \"alice@example.com\",   \"is_active\": False}\n",
    "]\n",
    "dataset_writer.write(updates_for_existing_rows)\n",
    "print(\"Updated existing rows (id=1,2,3) with new columns (email, is_active).\")\n",
    "\n",
    "# Write dataset data/metadata into feather files.\n",
    "dataset_writer.flush()\n",
    "print(\"Flushed all changes to the dataset.\")"
   ],
   "id": "c5a5bd6b6b41aab9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Step 5: Read data from feather file.",
   "id": "4c04ff3e526059f8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"\\nFinal dataset (merged from Parquet + Feather):\")\n",
    "for record in dataset.scan().to_pydict():\n",
    "    print(record)"
   ],
   "id": "81418b37c9de8692",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
